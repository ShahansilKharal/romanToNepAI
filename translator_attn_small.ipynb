{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Tu7dHLbS-6EJ"
      },
      "outputs": [],
      "source": [
        "import pdb\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sentencepiece as spm\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "h3zKMk1dVkPA"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PsZJqRgsVq8W"
      },
      "outputs": [],
      "source": [
        "nepali_tokenizer = spm.SentencePieceProcessor(\"spm_files/nepali_tokenizer.model\")\n",
        "roman_tokenizer = spm.SentencePieceProcessor(\"spm_files/roman_tokenizer.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "W0VzYUQEVvyO"
      },
      "outputs": [],
      "source": [
        "#@title loading data\n",
        "def file_to_df(filename):\n",
        "  train_data = open(filename,\"r\",encoding=\"utf8\").read()\n",
        "  train_data_lines = train_data.split(\"\\n\")\n",
        "  input_data = []\n",
        "  target_data=[]\n",
        "  for line in train_data_lines:\n",
        "    input,target = line.split(\"\\t\")\n",
        "    input_data.append(input)\n",
        "    target_data.append(target)\n",
        "\n",
        "  return pd.DataFrame({\"input\":input_data,\"target\":target_data})\n",
        "\n",
        "train_data_df = file_to_df(\"very_small_train_data.txt\")\n",
        "valid_data_df=file_to_df(\"very_small_valid_data.txt\")\n",
        "\n",
        "PAD_TOKEN = nepali_tokenizer.pad_id()\n",
        "def pad_and_remove_longer_tokens(row):\n",
        "  input = row.input\n",
        "  target = row.target\n",
        "  if(len(input) <=100 and len(target)<=100):\n",
        "        input = input + [PAD_TOKEN] * (100 - len(input))\n",
        "        target = target + [PAD_TOKEN] * (100 - len(target))\n",
        "        return input,target\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "\n",
        "EOS_TOKEN = nepali_tokenizer.eos_id()\n",
        "def tokenize_in_roman(data):\n",
        "  return roman_tokenizer.Encode(data)\n",
        "\n",
        "def tokenize_in_nepali(data):\n",
        "  return nepali_tokenizer.Encode(data) + [EOS_TOKEN]\n",
        "\n",
        "\n",
        "train_input_tokens = train_data_df[\"input\"].apply(tokenize_in_roman)\n",
        "train_target_tokens = train_data_df[\"target\"].apply(tokenize_in_nepali)\n",
        "train_tokens_df = pd.DataFrame({\"input\":train_input_tokens,\"target\":train_target_tokens})\n",
        "\n",
        "valid_input_tokens=valid_data_df[\"input\"].apply(tokenize_in_roman)\n",
        "valid_target_tokens=valid_data_df[\"target\"].apply(tokenize_in_nepali)\n",
        "valid_tokens_df = pd.DataFrame({\"input\":valid_input_tokens,\"target\":valid_target_tokens})\n",
        "\n",
        "train_short_tokens=train_tokens_df.apply(pad_and_remove_longer_tokens,axis=1)\n",
        "train_short_tokens = train_short_tokens.dropna()\n",
        "train_short_tokens_array = train_short_tokens.values\n",
        "\n",
        "valid_short_tokens=valid_tokens_df.apply(pad_and_remove_longer_tokens,axis=1)\n",
        "valid_short_tokens = valid_short_tokens.dropna()\n",
        "valid_short_tokens_array = valid_short_tokens.values\n",
        "\n",
        "train_small_tokens_input_list=[]\n",
        "train_small_tokens_target_list=[]\n",
        "for value in train_short_tokens_array:\n",
        "  train_small_tokens_input_list.append(value[0])\n",
        "  train_small_tokens_target_list.append(value[1])\n",
        "\n",
        "train_small_tokens_df = pd.DataFrame({\"input\":train_small_tokens_input_list,\"target\":train_small_tokens_target_list})\n",
        "\n",
        "valid_small_tokens_input_list=[]\n",
        "valid_small_tokens_target_list=[]\n",
        "for value in valid_short_tokens_array:\n",
        "  valid_small_tokens_input_list.append(value[0])\n",
        "  valid_small_tokens_target_list.append(value[1])\n",
        "\n",
        "valid_small_tokens_df = pd.DataFrame({\"input\":valid_small_tokens_input_list,\"target\":valid_small_tokens_target_list})\n",
        "class trainDataset(Dataset):\n",
        "  def __init__(self,df):\n",
        "    super().__init__()\n",
        "    self.input = df[\"input\"].values\n",
        "    self.target=df[\"target\"].values\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return (torch.tensor(self.input[index]),torch.tensor(self.target[index]))\n",
        "\n",
        "def split_dataset(df):\n",
        "  train_data ,test_data = train_test_split(train_small_tokens_df,train_size=0.8,shuffle=True)\n",
        "  train_data=train_data.reset_index()\n",
        "  test_data=test_data.reset_index()\n",
        "\n",
        "  train_dataset = trainDataset(train_data)\n",
        "  test_dataset = trainDataset(test_data)\n",
        "\n",
        "  return train_dataset,test_dataset\n",
        "\n",
        "def split_dataloader(train_dataset,test_dataset,batch_size=64):\n",
        "  train_dataloader = DataLoader(train_dataset,batch_size=batch_size)\n",
        "  test_dataloader = DataLoader(test_dataset,batch_size=batch_size,shuffle=True)\n",
        "  return train_dataloader,test_dataloader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tNd5pTJgV1ME"
      },
      "outputs": [],
      "source": [
        "train_dataset , test_dataset = split_dataset(train_small_tokens_df)\n",
        "valid_dataset =  trainDataset(valid_small_tokens_df)\n",
        "\n",
        "\n",
        "train_dataloader, test_dataloader = split_dataloader(train_dataset,test_dataset)\n",
        "valid_dataloader = DataLoader(valid_dataset,batch_size=64,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HriYR0PFV3pd"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 100\n",
        "VOCAB_SIZE = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2xGmzD_WV5YC"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size,hidden_size,dropout_p=0.1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, hidden = self.gru(embedded)\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lcnXF7mXGdMx"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Va = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, query, keys):\n",
        "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
        "        scores = scores.squeeze(2).unsqueeze(1)\n",
        "\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "        context = torch.bmm(weights, keys)\n",
        "\n",
        "        return context, weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RpZpedQOV7th"
      },
      "outputs": [],
      "source": [
        "SOS_TOKEN= nepali_tokenizer.bos_id()\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size,dropout_p=0.1):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.attention = BahdanauAttention(hidden_size)\n",
        "        self.gru = nn.GRU(2*hidden_size, hidden_size,batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None,teacher_forcing_ratio=0.5):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long,device=device).fill_(SOS_TOKEN)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "        attentions = []\n",
        "\n",
        "        for i in range(MAX_LENGTH):\n",
        "            decoder_output, decoder_hidden, attn_weights  = self.forward_step(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_outputs.append(decoder_output)\n",
        "            attentions.append(attn_weights)\n",
        "\n",
        "            if target_tensor is not None and random.random() < teacher_forcing_ratio:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                  decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        attentions = torch.cat(attentions, dim=1)\n",
        "        return decoder_outputs, decoder_hidden, attentions # We return `None` for consistency in the training loop\n",
        "\n",
        "    def forward_step(self, input, hidden,encoder_outputs):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        query = hidden.permute(1,0,2)\n",
        "        context, attn_weights = self.attention(query, encoder_outputs)\n",
        "        input_gru = torch.cat((embedded, context), dim=2)\n",
        "        output, hidden = self.gru(input_gru, hidden)\n",
        "        output = self.out(output)\n",
        "        # breakpoint()\n",
        "        return output, hidden, attn_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CgL1mhk8V9qG"
      },
      "outputs": [],
      "source": [
        "encoder = EncoderRNN(VOCAB_SIZE, 256).to(device)\n",
        "decoder = DecoderRNN(256, VOCAB_SIZE).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kLqX9ZABV_VS"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "8CFit9IMWBd_"
      },
      "outputs": [],
      "source": [
        "def train_step(input_tensor, target_tensor, encoder, decoder, optimizer, criterion, teacher_forcing_ratio=0):\n",
        "    encoder_hidden = None  # Initial hidden state is None for GRU\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    input_tensor = input_tensor.to(device)\n",
        "    target_tensor = target_tensor.to(device)\n",
        "\n",
        "    encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "\n",
        "    decoder_output, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
        "\n",
        "    loss = criterion(decoder_output.view(-1, VOCAB_SIZE), target_tensor.view(-1))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4AqGaAZYWDTj"
      },
      "outputs": [],
      "source": [
        "def train(encoder, decoder, train_dataloader, optimizer, criterion, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for input_tensor, target_tensor in train_dataloader:\n",
        "            loss = train_step(input_tensor, target_tensor, encoder, decoder, optimizer, criterion)\n",
        "            total_loss += loss\n",
        "\n",
        "        avg_loss = total_loss / len(train_dataloader)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "VMq0VkGMWFca"
      },
      "outputs": [],
      "source": [
        "def evaluate(encoder, decoder, dataloader, criterion):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for input_tensor, target_tensor in dataloader:\n",
        "            input_tensor = input_tensor.to(device)\n",
        "            target_tensor = target_tensor.to(device)\n",
        "\n",
        "            encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "            decoder_output, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
        "\n",
        "            loss = criterion(decoder_output.view(-1, VOCAB_SIZE), target_tensor.view(-1))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Validation Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    encoder.train()\n",
        "    decoder.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "DgINwa31WHzd",
        "outputId": "fb911432-6844-444d-e9d5-10f12639c668"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[-0.0515, -0.3681,  0.4504,  ...,  0.0201,  0.0135,  0.2471]],\n",
            "\n",
            "        [[-0.2960, -0.3298,  0.1043,  ..., -0.7728,  0.5003, -0.1448]],\n",
            "\n",
            "        [[-0.7270,  0.0708, -0.4237,  ...,  0.4888,  0.5825, -0.0470]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.6543, -0.4009, -0.2980,  ..., -0.3993, -0.0213, -0.5545]],\n",
            "\n",
            "        [[ 0.3581, -0.7107,  0.2262,  ...,  0.7035, -0.2542,  0.5541]],\n",
            "\n",
            "        [[ 0.5336, -0.0025,  0.2920,  ..., -0.4206, -0.0582, -0.1488]]],\n",
            "       grad_fn=<BmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 30  # Adjust as needed\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train(encoder, decoder, train_dataloader, optimizer, criterion, num_epochs=1)\n",
        "    evaluate(encoder, decoder, valid_dataloader, criterion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "L7IP7XsFgyij"
      },
      "outputs": [],
      "source": [
        "# torch.save(encoder.state_dict(), \"/content/drive/MyDrive/very_small_encoder_bpe.pth\")\n",
        "# torch.save(decoder.state_dict(), \"/content/drive/MyDrive/very_small_decoder_bpe.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TriFYbU2YPg"
      },
      "source": [
        "Loading saved model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1-LqHUSoQJr",
        "outputId": "976d2e84-4725-40a6-f4a3-c4bcab94ea8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoder.load_state_dict(torch.load(\"very_small_encoder_bpe.pth\",map_location=torch.device('cpu')))\n",
        "decoder.load_state_dict(torch.load(\"very_small_decoder_bpe.pth\",map_location=torch.device(\"cpu\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "wxISS3Kwafrr",
        "outputId": "be16fd65-5621-469b-a869-064ba3fb95b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'ती ती ती ती हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो हो'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sent = \"timi ko ho\"\n",
        "tokens = roman_tokenizer.Encode(sent)\n",
        "input_tensor = torch.tensor(tokens).unsqueeze(0).to(device)\n",
        "encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "decoder_output, _, _ = decoder(encoder_outputs, encoder_hidden)\n",
        "predicted_indices = torch.argmax(decoder_output[0], dim=1)\n",
        "predicted_tokens = nepali_tokenizer.Decode(predicted_indices.tolist())\n",
        "predicted_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.cpu().numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
        "                       ['<EOS>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def evaluateAndShowAttention(input_sentence):\n",
        "    output_words, attentions = evaluate(encoder, decoder, valid_dataloader,criterion)\n",
        "    print('input =', input_sentence)\n",
        "    print('output =', ' '.join(output_words))\n",
        "    showAttention(input_sentence, output_words, attentions[0, :len(output_words), :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'तिम्रो घर कहाँ थियो मलाई नसोध'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sent = \"timro ghar kaha thiyo malai nasodha\"\n",
        "tokens = roman_tokenizer.Encode(sent) \n",
        "tokens= tokens + [PAD_TOKEN]*(100-len(tokens))\n",
        "input_tensor = torch.tensor(tokens).unsqueeze(0).to(device)\n",
        "encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "decoder_output, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
        "_, topi = decoder_output[0].topk(1)\n",
        "decoded_ids = topi.squeeze()\n",
        "decoded_tokens = nepali_tokenizer.Decode(decoded_ids.tolist())\n",
        "decoded_tokens"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
